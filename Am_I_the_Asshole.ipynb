{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Am I the asshole?",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h14wiDWAXBbx"
      },
      "source": [
        "## **Project Brief:**\n",
        "-stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf8IfHmDXjAj"
      },
      "source": [
        "## Part 0.5 - Trial Using Kaggle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcd7Y6BLL2Ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470ae37b-0d9d-4544-f9ba-dfb5fec0caa2"
      },
      "source": [
        "!gdown --id 1cCrLkMt8oGUX9p1LPmxinZh7YiaSLQ-s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cCrLkMt8oGUX9p1LPmxinZh7YiaSLQ-s\n",
            "To: /content/AITAKaggle.csv\n",
            "100% 165M/165M [00:01<00:00, 135MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "and3KXT5XrZZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "42b5acf4-849d-4e71-c43b-03fa3f14a063"
      },
      "source": [
        "#Download dataset and display 5 random posts and their scores\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/AITAKaggle.csv\")\n",
        "\n",
        "print(\"There are\", len(df.index), \"files in dataset.\")\n",
        "print(\"There are\", len(df[df[\"is_asshole\"] == True]), \"NTA-rated posts.\")\n",
        "print(\"There are\", len(df[df[\"is_asshole\"] == False]), \"YTA-rated posts.\")\n",
        "df.sample(n = 5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 91488 files in dataset.\n",
            "There are 18265 NTA-rated posts.\n",
            "There are 73223 YTA-rated posts.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>is_asshole</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>76740</th>\n",
              "      <td>AITA for telling my boyfriend it was my sister...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41825</th>\n",
              "      <td>AITA For throwing away something backstage.\\n(...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63664</th>\n",
              "      <td>AITA for not wanting to pay half of a utility ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83537</th>\n",
              "      <td>AITA for not buying my brother chips because i...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13471</th>\n",
              "      <td>WIBTA If I Asked My Friend to Not Come on Vaca...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    body  is_asshole\n",
              "76740  AITA for telling my boyfriend it was my sister...         0.0\n",
              "41825  AITA For throwing away something backstage.\\n(...         0.0\n",
              "63664  AITA for not wanting to pay half of a utility ...         0.0\n",
              "83537  AITA for not buying my brother chips because i...         0.0\n",
              "13471  WIBTA If I Asked My Friend to Not Come on Vaca...         1.0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9i7ENTdYcY2",
        "outputId": "47828e80-88f5-4059-81f8-7c5cea561c30"
      },
      "source": [
        "text_list = \". \".join(df[\"body\"])\n",
        "print(text_list[:400])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AITA for not apologising for who I’m even if my personality bugs others opinions or perspectives?\n",
            "It’s not just one instance. There have been many like these over the years.\n",
            "I’m a women in my mid twenties. When I was young like before teenage, I was a tomboy. During teenage I became a bit feminine but after teenage, when I started figuring myself out and understood myself more, I became a tomboy a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50_xIyd_Y-Jv",
        "outputId": "af6dbc8f-a718-4a86-f880-cb21e4b3c6f9"
      },
      "source": [
        "word_list = text_list.split()\n",
        "print(word_list[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AITA', 'for', 'not', 'apologising', 'for', 'who', 'I’m', 'even', 'if', 'my', 'personality', 'bugs', 'others', 'opinions', 'or', 'perspectives?', 'It’s', 'not', 'just', 'one', 'instance.', 'There', 'have', 'been', 'many', 'like', 'these', 'over', 'the', 'years.', 'I’m', 'a', 'women', 'in', 'my', 'mid', 'twenties.', 'When', 'I', 'was', 'young', 'like', 'before', 'teenage,', 'I', 'was', 'a', 'tomboy.', 'During', 'teenage', 'I', 'became', 'a', 'bit', 'feminine', 'but', 'after', 'teenage,', 'when', 'I', 'started', 'figuring', 'myself', 'out', 'and', 'understood', 'myself', 'more,', 'I', 'became', 'a', 'tomboy', 'again.', 'I’ve', 'pixie', 'hair', 'and', 'my', 'attire', 'is', 'generally', 'formal', 'no', 'matter', 'where', 'I’m.', 'What', 'has', 'happened', 'is', 'that', 'my', 'family,', 'paternal', 'cousins', 'and', 'aunts', 'think', 'that', 'I’m']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "BUpFRQWmZV97",
        "outputId": "034b5f3e-2b6a-4841-88cf-f452e33c0341"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "word_list_no_stops = [word for word in word_list if word not in stopwords.words(\"english\")]\n",
        "\n",
        "print(word_list_no_stops[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-aef2955798ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword_list_no_stops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list_no_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-aef2955798ec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword_list_no_stops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list_no_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuUzmfhB4EjR",
        "outputId": "520829e9-575b-4c85-f997-58a57f196183"
      },
      "source": [
        "#Find and display the most frequent words in different subsections of data\n",
        "print('The 10 most frequent words in the entire corpus are:')\n",
        "print(pd.Series(' '.join(df['body']).lower().split()).value_counts()[:10])\n",
        "print('The 10 most frequent words in \"non-asshole\" posts are:')\n",
        "print(pd.Series(' '.join(df.loc[df['is_asshole'] == 0.0, 'body']).lower().split()).value_counts()[:10])\n",
        "print('The 10 most frequent words in \"asshole\" posts are:')\n",
        "print(pd.Series(' '.join(df.loc[df['is_asshole'] == 1.0, 'body']).lower().split()).value_counts()[:10])\n",
        "\"\"\"The one thing I notice about this that could be interesting is the fact that \n",
        "\"she\" is the 7th most used word in posts rated YTA. By a fairly significant\n",
        "margin, as well. It is 10th in the general dataset and doesn't make the top 10\n",
        "at all in NTA posts. Perhaps that's indicating the fact that r/AITA users more \n",
        "often side with women?\"\"\"\n",
        "\n",
        "\n",
        "#Below are some random things I did trying to get the above to work :sweat_smile:\n",
        "#pd.Series(' '.join(df.loc[df['is_asshole'] == 1.0, 'body'].iloc[0]).lower().split()).value_counts()[:10]\n",
        "#pd.Series(' '.join(df['is_asshole'] == 0.0).lower().split()).value_counts()[:10]\n",
        "#df.loc[df['body'], ['is_asshole'] == 0.0]\n",
        "#df.loc[df['is_asshole'] == 1.0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most frequent words in the entire corpus are:\n",
            "i       1334415\n",
            "and     1109355\n",
            "to      1015208\n",
            "the      863185\n",
            "my       733101\n",
            "a        680006\n",
            "for      419777\n",
            "that     405211\n",
            "was      404672\n",
            "she      392939\n",
            "dtype: int64\n",
            "The most frequent words in \"non-asshole\" posts are:\n",
            "i       1090585\n",
            "and      908555\n",
            "to       818958\n",
            "the      698550\n",
            "my       610694\n",
            "a        555821\n",
            "for      342890\n",
            "was      330323\n",
            "that     322539\n",
            "of       315842\n",
            "dtype: int64\n",
            "The most frequent words in \"asshole\" posts are:\n",
            "i       243830\n",
            "and     200800\n",
            "to      196250\n",
            "the     164635\n",
            "a       124185\n",
            "my      122407\n",
            "she      83895\n",
            "that     82672\n",
            "for      76887\n",
            "of       74576\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYSM6NQ7ZU73"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VErjzmAuXQYp"
      },
      "source": [
        "## Part 1 - Gathering the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht-6J42RWfHj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN0xQIcAXsJi"
      },
      "source": [
        "## Part 2 - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fstIwduYX16_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z1dLTtvX2c3"
      },
      "source": [
        "## Part 3 -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3RmuZKQX6J6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYhVoiKUX6va"
      },
      "source": [
        "## Part ? - Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwY6FN5fX-g7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZK-hceuX_i-"
      },
      "source": [
        "## Conclusion and Takeaways"
      ]
    }
  ]
}